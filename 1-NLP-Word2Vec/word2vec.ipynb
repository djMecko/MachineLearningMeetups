{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24fb76d",
   "metadata": {},
   "source": [
    "# Aplicando Word2Vec usando BlazingText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c7593",
   "metadata": {},
   "source": [
    "Word2Vec es un algoritmo popular que se utiliza para generar representaciones vectoriales densas de palabras en grandes corpus mediante el aprendizaje no supervisado. Estas representaciones son útiles para muchas tareas de procesamiento del lenguaje natural (NLP) como el análisis de sentimientos, el reconocimiento de entidades con nombre y la traducción automática."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d97fa2d",
   "metadata": {},
   "source": [
    "Para el siguiente ejemplo usaremos como guia los siguientes articulos:\n",
    "- Guia de ejemplos oficial de AWS. [link](https://github.com/aws/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/blazingtext_word2vec_subwords_text8/blazingtext_word2vec_subwords_text8.ipynb?short_path=27c95ac)\n",
    "- Entrenamiento y evaluación de un modelo Word2Vec usando BlazingText en Sagemaker. [link](https://t-redactyl.io/blog/2020/09/training-and-evaluating-a-word2vec-model-using-blazingtext-in-sagemaker.html)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216553f8",
   "metadata": {},
   "source": [
    "## Configuracion inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a9cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)# Este es el rol que SageMaker usaría para acceder a los recursos de AWS (S3, CloudWatch) en su nombre\n",
    "\n",
    "bucket = sess.default_bucket()  # Reemplácelo con el nombre de su bucket si no se creara uno por defecto\n",
    "print(bucket)\n",
    "prefix = \"blazingtext/subwords\"  # Reemplace con el prefijo con el cual desea almacenar los datos si es necesario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c32220",
   "metadata": {},
   "source": [
    "## Ingestion de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c71fe19",
   "metadata": {},
   "source": [
    "A continuación, descargamos un conjunto de datos de la web con los que queremos entrenar los vectores de palabras. BlazingText espera un solo archivo de texto preprocesado con tokens separados por espacios y cada línea del archivo debe contener una sola oración.\n",
    "\n",
    "En este ejemplo, entrenemos los vectores en el conjunto de datos text8 (100 MB), que es una versión pequeña (ya preprocesada) de un respaldo de Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargamos\n",
    "!wget http://mattmahoney.net/dc/text8.zip -O text8.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2d6d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomprimimos\n",
    "!gzip -d text8.gz -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638ef7b",
   "metadata": {},
   "source": [
    "Una vez completada la descarga y descompresión de datos, debemos cargarlos en S3 para que SageMaker pueda consumirlos para ejecutar un trabajo de entrenamiento. Usaremos Python SDK para cargar estos dos archivos en el depósito y la ubicación del prefijo que hemos establecido anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603cc680",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = prefix + \"/train\"\n",
    "\n",
    "sess.upload_data(path=\"text8\", bucket=bucket, key_prefix=train_channel)\n",
    "\n",
    "s3_train_data = \"s3://{}/{}\".format(bucket, train_channel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04877254",
   "metadata": {},
   "source": [
    "A continuación, debemos configurar una ubicación de salida en S3, donde se descargará el artefacto del modelo. Estos artefactos también son el resultado del trabajo de entrenamiento del algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25faa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = \"s3://{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009b875",
   "metadata": {},
   "source": [
    "## Configuración de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206dc196",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78dc556",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print(\"Using SageMaker BlazingText container: {} ({})\".format(container, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.c4.2xlarge\",  # Se recomienda encarecidamente el uso de ml.p3.2xlarge para lograr la máxima velocidad y rentabilidad\n",
    "    train_volume_size=30,\n",
    "    train_max_run=360000,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85954653",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(\n",
    "    mode=\"skipgram\",\n",
    "    epochs=5,\n",
    "    min_count=5,\n",
    "    sampling_threshold=0.0001,\n",
    "    learning_rate=0.05,\n",
    "    window_size=5,\n",
    "    vector_dim=100,\n",
    "    negative_samples=5,\n",
    "    subwords=True,\n",
    "    min_char=3,\n",
    "    max_char=6,\n",
    "    batch_size=11,\n",
    "    evaluation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48892881",
   "metadata": {},
   "source": [
    "Ahora que los hiperparámetros están configurados, preparemos la configuracion entre nuestros canales de datos y el algoritmo. Para hacer esto, necesitamos crear los objetos sagemaker.session.s3_input desde nuestros canales de datos. Luego, estos objetos se colocan en un diccionario simple, que consume el algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e81c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(\n",
    "    s3_train_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "data_channels = {\"train\": train_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a361b0c5",
   "metadata": {},
   "source": [
    "Una vez finalizado el trabajo, se imprimirá el mensaje \"Trabajo completado\". El modelo entrenado se puede encontrar en el depósito de S3 que se configuró como output_path en el estimador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22487389",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf89ab",
   "metadata": {},
   "source": [
    "## Hospedaje / Inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd53f0",
   "metadata": {},
   "source": [
    "Una vez finalizada la capacitación, podemos implementar el modelo entrenado como un punto de enlace alojado en tiempo real de Amazon SageMaker. Esto nos permitirá hacer predicciones (o inferencias) a partir del modelo. Tenga en cuenta que no tenemos que alojar en el mismo tipo de instancia que solíamos entrenar. Debido a que los puntos finales de instancia estarán en funcionamiento durante mucho tiempo, es recomendable elegir una instancia más barata para la inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac6bf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_endpoint = bt_model.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf5145b",
   "metadata": {},
   "source": [
    "### Usar formato JSON para inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92955a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"awesome\", \"hello\"]\n",
    "\n",
    "payload = {\"instances\": words}\n",
    "\n",
    "response = bt_endpoint.predict(json.dumps(payload), initial_args={'ContentType': 'application/json'})\n",
    "vecs = json.loads(response)\n",
    "print(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985fc8b1",
   "metadata": {},
   "source": [
    "## Visual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a714fa",
   "metadata": {},
   "source": [
    "Descargamos nuestro modelo desde S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f6f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://sagemaker-us-east-1-{account_number}/blazingtext/subwords/output/blazingtext-{bucket-postfix}/output/model.tar.gz - | tar -xz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58788537",
   "metadata": {},
   "source": [
    "Instalamos la libreria gensim para manejar los vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c2a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922b55b1",
   "metadata": {},
   "source": [
    "Importamos la clase KeyedVectors y cargamos los vectores a consultar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f60baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e1f5b0",
   "metadata": {},
   "source": [
    "Realizamos operaciones con vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(\"family\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56993464",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf0aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.distance(\"media\", \"media\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d4068e",
   "metadata": {},
   "outputs": [],
   "source": [
    " word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9dc5a9",
   "metadata": {},
   "source": [
    "Cargamos las librerias para la reduccion de dimencionalidad de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb20c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pylab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ece4f4f",
   "metadata": {},
   "source": [
    "## Generamos una grafica con todos los vectores en 2 dimenciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 400\n",
    "\n",
    "first_line = True\n",
    "index_to_word = []\n",
    "with open(\"vectors.txt\",\"r\") as f:\n",
    "    for line_num, line in enumerate(f):\n",
    "        if first_line:\n",
    "            dim = int(line.strip().split()[1])\n",
    "            word_vecs = np.zeros((num_points, dim), dtype=float)\n",
    "            first_line = False\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        word = line.split()[0]\n",
    "        vec = word_vecs[line_num-1]\n",
    "        for index, vec_val in enumerate(line.split()[1:]):\n",
    "            vec[index] = float(vec_val)\n",
    "        index_to_word.append(word)\n",
    "        if line_num >= num_points:\n",
    "            break\n",
    "word_vecs = normalize(word_vecs, copy=False, return_norm=False)\n",
    "\n",
    "tsne = TSNE(perplexity=40, n_components=2, init='pca', n_iter=10000)\n",
    "two_d_embeddings = tsne.fit_transform(word_vecs[:num_points])\n",
    "labels = index_to_word[:num_points]\n",
    "\n",
    "def plot(embeddings, labels):\n",
    "    pylab.figure(figsize=(20,20))\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                       ha='right', va='bottom')\n",
    "    pylab.savefig('tsne_blazing_text_initial.png', bbox_inches='tight')\n",
    "\n",
    "plot(two_d_embeddings, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
